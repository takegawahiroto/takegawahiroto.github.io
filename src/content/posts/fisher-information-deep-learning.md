---
title: "Fisher情報行列と深層学習の幾何学"
date: "2026-02-01"
tags: ["Mathematics", "Deep Learning"]
excerpt: "パラメータ空間上のRiemann計量としてのFisher情報行列。自然勾配法の理論的基盤を情報幾何の視点から掘り下げる。"
category: "Math"
---

## はじめに

深層学習のパラメータ最適化を考えるとき、私たちは通常ユークリッド空間上の勾配降下法を用いる。しかし、パラメータ空間には自然なRiemann計量が存在する — それがFisher情報行列である。

## Fisher情報行列とは

確率モデル $p(x|\theta)$ が与えられたとき、Fisher情報行列は以下で定義される：

$$
F(\theta) = \mathbb{E}\left[\nabla \log p(x|\theta) \cdot \nabla \log p(x|\theta)^\top\right]
$$

この行列は、パラメータ空間上の計量テンソルとして解釈できる。つまり、パラメータの微小変化がモデルの出力分布にどれだけ影響を与えるかを測っている。

## 自然勾配法

通常の勾配降下法では、パラメータ空間のユークリッド的な距離に基づいて更新を行う。しかし、Fisher情報行列を計量として用いることで、分布空間上のKLダイバージェンスに基づく更新が可能になる。

これが **自然勾配法** であり、更新則は次のようになる：

$$
\theta_{t+1} = \theta_t - \eta \, F(\theta)^{-1} \nabla L(\theta)
$$

## 情報幾何との接続

甘利俊一による情報幾何学の枠組みでは、統計モデルの族は微分多様体をなし、Fisher情報行列はその上のRiemann計量を定義する。この視点から見ると、自然勾配法は多様体上の最急降下法に他ならない。

## 実装上の課題

Fisher情報行列の逆行列の計算は、パラメータ数が $N$ のとき $O(N^3)$ の計算量を要する。深層学習モデルでは $N$ が数百万に達するため、直接計算は非現実的である。

K-FAC (Kronecker-Factored Approximate Curvature) などの近似手法が提案されており、実用的な自然勾配法の実現に向けた研究が活発に行われている。

## まとめ

Fisher情報行列は、統計学・情報理論・微分幾何学を結ぶ美しい概念である。深層学習への応用はまだ発展途上だが、最適化の理論的理解を深めるうえで欠かせない道具となっている。
